{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prkrptr/colab_nbs/blob/main/Week_2_Five_V's_of_Big_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# DAT204M - Week 1: Five V's of Big Data\n",
        "#\n",
        "# This assignment is designed to help you practice basic data processing\n",
        "# in Python, a fundamental skill for working with big data. You will work\n",
        "# with different \"types\" of data and perform simple analysis.\n",
        "#\n",
        "# Remember the five Vs of big data: Volume, Velocity, Veracity, Value, and Variety.\n",
        "# This assignment focuses on Variety, Veracity, and data processing for Value.\n",
        "# ==============================================================================\n",
        "\n",
        "import json\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# new imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# --- Part 1: Generating Synthetic Data ---\n",
        "\n",
        "def generate_structured_data(num_records):\n",
        "    \"\"\"\n",
        "    Generates a list of dictionaries simulating structured event data.\n",
        "    \"\"\"\n",
        "    event_types = [\"page_view\", \"click\", \"purchase\"]\n",
        "    records = []\n",
        "    for i in range(num_records):\n",
        "        user_id = random.randint(1000, 2000)\n",
        "        event_type = random.choice(event_types)\n",
        "        timestamp = (datetime.utcnow() - timedelta(seconds=random.randint(0, 3600))).isoformat() + \"Z\"\n",
        "        records.append({\"user_id\": user_id, \"event_type\": event_type, \"timestamp\": timestamp})\n",
        "    return records\n",
        "\n",
        "def generate_semi_structured_data(num_records):\n",
        "    \"\"\"\n",
        "    Generates a list of JSON-like strings.\n",
        "    Intentionally includes some malformed data to test Veracity handling.\n",
        "    \"\"\"\n",
        "    content_snippets = [\n",
        "        \"The quick brown fox jumped over the lazy dog.\",\n",
        "        \"Python is a powerful programming language.\",\n",
        "        \"Big data is characterized by the 5 Vs.\",\n",
        "        \"Data can be structured, unstructured, or semi-structured.\",\n",
        "        \"The big data ecosystem is complex.\",\n",
        "        \"DataOps and governance are important.\",\n",
        "        \"AI technologies are used to analyze data.\",\n",
        "    ]\n",
        "    tags_list = [\n",
        "        [\"animal\", \"story\"],\n",
        "        [\"programming\", \"python\"],\n",
        "        [\"big data\", \"lecture\", \"5vs\"],\n",
        "        [\"data\", \"types\"],\n",
        "        [\"big data\", \"ecosystem\", \"trends\"],\n",
        "        [\"data\", \"governance\", \"trends\"],\n",
        "        [\"AI\", \"analytics\", \"learning\"],\n",
        "    ]\n",
        "\n",
        "    records = []\n",
        "    for i in range(num_records):\n",
        "        if i % 10 == 0:  # Introduce a malformed record every 10 records\n",
        "            records.append('This is not valid JSON.')\n",
        "            continue\n",
        "\n",
        "        doc_id = str(uuid.uuid4())\n",
        "        content = random.choice(content_snippets)\n",
        "        tags = random.choice(tags_list)\n",
        "        records.append(json.dumps({\"id\": doc_id, \"content\": content, \"tags\": tags}))\n",
        "    return records\n",
        "\n",
        "def generate_unstructured_data(num_records):\n",
        "    \"\"\"\n",
        "    Generates a list of simple sentences.\n",
        "    \"\"\"\n",
        "    lecture_keywords = [\"data\", \"structured\", \"unstructured\", \"semi-structured\", \"volume\", \"velocity\", \"variety\", \"mining\", \"DataOps\", \"learning\", \"AI\", \"analytics\", \"trends\"]\n",
        "    sentences = []\n",
        "    for _ in range(num_records):\n",
        "        num_keywords = random.randint(1, 3)\n",
        "        sentence_parts = [random.choice(lecture_keywords) for _ in range(num_keywords)]\n",
        "        # Add some filler words to make it more like a sentence\n",
        "        filler_words = [\"is\", \"the\", \"a\", \"and\", \"or\", \"in\", \"with\", \"from\"]\n",
        "        random.shuffle(filler_words)\n",
        "        combined_words = []\n",
        "        for word in sentence_parts:\n",
        "            combined_words.append(word)\n",
        "            if random.random() > 0.5: # Add a filler word sometimes\n",
        "                combined_words.append(random.choice(filler_words))\n",
        "\n",
        "        sentence = \" \".join(combined_words)\n",
        "        sentence = sentence.capitalize() + \".\"\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "# --- Part 2: Assignment Tasks ---\n",
        "# Note: These functions are the same as before and will now operate on much larger datasets.\n",
        "\n",
        "def count_user_events(data):\n",
        "    \"\"\"\n",
        "    Counts 'click' and 'page_view' events for each user.\n",
        "    Returns a dictionary with user_id as key and event counts as value.\n",
        "    Example output: {101: {'page_view': 1, 'click': 2}}\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    grouped = df.groupby(['user_id', 'event_type']).size() # Group by the user_id and event_type and then count the rows per group\n",
        "    grouped = grouped.unstack(fill_value=0) # pivot the event_types to column and fills in the missing with 0\n",
        "    user_event_counts= grouped.to_dict(\"index\") # turn into dictiionary with user_id as the index\n",
        "\n",
        "    # user_event_counts = {}\n",
        "    # for record in data:\n",
        "    #     user_id = record['user_id']\n",
        "    #     event_type = record['event_type']\n",
        "    #     if event_type in ['click', 'page_view']:\n",
        "    #         if user_id not in user_event_counts:\n",
        "    #             user_event_counts[user_id] = {'page_view': 0, 'click': 0}\n",
        "    #         user_event_counts[user_id][event_type] += 1\n",
        "    return user_event_counts\n",
        "\n",
        "def find_multi_tagged_documents(data):\n",
        "    \"\"\"\n",
        "    Safely parses JSON strings and finds documents that have more than one tag.\n",
        "    Returns a list of dictionaries for each valid document found.\n",
        "    \"\"\"\n",
        "    parsed_json_docs = []\n",
        "    for record_str in data:\n",
        "      try:\n",
        "        record = json.loads(record_str)\n",
        "        parsed_json_docs.append(record)\n",
        "      except json.JSONDecodeError:\n",
        "        continue # skip\n",
        "\n",
        "    df = pd.DataFrame(parsed_json_docs)\n",
        "    # filter df with more than one tags\n",
        "    multi_tagged_df = df[df[\"tags\"].map(len) > 1]\n",
        "\n",
        "    # Convert filtered df to a list of dictionaries\n",
        "    valid_docs_df = multi_tagged_df.to_dict(orient=\"records\")\n",
        "    return valid_docs_df\n",
        "\n",
        "\n",
        "    # for record_str in data:\n",
        "    #     try:\n",
        "    #         record = json.loads(record_str)\n",
        "    #         if len(record.get('tags', [])) > 1:\n",
        "    #             valid_documents.append({\"id\": record['id'], \"tags\": record['tags']})\n",
        "    #     except json.JSONDecodeError:\n",
        "    #         print(f\"Skipping malformed JSON record: '{record_str}'\")\n",
        "    # return valid_documents\n",
        "\n",
        "def find_most_frequent_keyword(data):\n",
        "    \"\"\"\n",
        "    Finds the most frequent keyword from the lecture in the unstructured data.\n",
        "    Returns a tuple of the keyword and its count.\n",
        "    Example: ('data', 3)\n",
        "    \"\"\"\n",
        "    keywords = [\"data\", \"structured\", \"unstructured\", \"semi-structured\", \"volume\", \"velocity\", \"variety\", \"mining\", \"DataOps\", \"learning\", \"AI\", \"analytics\", \"trends\"]\n",
        "    # keyword_counts = {keyword: 0 for keyword in keywords}\n",
        "\n",
        "    vocab_keywords = [keyword.lower() for keyword in keywords]\n",
        "\n",
        "    # convert data into a matrix counting only the keywords\n",
        "    vectorizer = CountVectorizer(vocabulary = vocab_keywords, lowercase = True)\n",
        "    X = vectorizer.fit_transform(data)\n",
        "    # Sum each column to get total counts per keyword, flatten to 1D array\n",
        "    word_counts = np.squeeze(np.asarray(X.sum(axis=0)))\n",
        "\n",
        "    # Get the words in the same order as the columns\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # map keyword to their counts\n",
        "    freq = dict(zip(words, word_counts))\n",
        "    most_frequent = max(freq, key=freq.get)\n",
        "    return (most_frequent, freq[most_frequent])\n",
        "\n",
        "\n",
        "    # for sentence in data:\n",
        "    #     words = sentence.lower().split()\n",
        "    #     for word in words:\n",
        "    #         word = word.replace(\",\", \"\").replace(\".\", \"\")\n",
        "    #         if word in keyword_counts:\n",
        "    #             keyword_counts[word] += 1\n",
        "\n",
        "    # most_frequent = (\"\", 0)\n",
        "    # for keyword, count in keyword_counts.items():\n",
        "    #     if count > most_frequent[1]:\n",
        "    #         most_frequent = (keyword, count)\n",
        "\n",
        "    # return most_frequent\n",
        "\n",
        "# --- Part 3: Running the Assignment ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Instructions for students ---\n",
        "    print(\"--- Big Data Programming Assignment ---\")\n",
        "    print(\"The datasets are now being generated synthetically.\")\n",
        "    print(\"This simulates the 'Volume' characteristic of big data.\")\n",
        "\n",
        "    # Generate large datasets (e.g., 1000 records each)\n",
        "    NUM_RECORDS = 10000\n",
        "    structured_data = generate_structured_data(NUM_RECORDS)\n",
        "    semi_structured_data = generate_semi_structured_data(NUM_RECORDS)\n",
        "    unstructured_data = generate_unstructured_data(NUM_RECORDS)\n",
        "\n",
        "    print(f\"\\nSuccessfully generated {len(structured_data)} structured records.\")\n",
        "    print(f\"Successfully generated {len(semi_structured_data)} semi-structured records.\")\n",
        "    print(f\"Successfully generated {len(unstructured_data)} unstructured records.\")\n",
        "\n",
        "    print(\"\\n--- Your Results ---\")\n",
        "\n",
        "    # Run Task 1 and print the result.\n",
        "    user_event_counts = count_user_events(structured_data)\n",
        "    print(\"Task 1: User event counts for the first 5 users:\")\n",
        "    # Print a small subset to avoid overwhelming the output\n",
        "    for user_id, counts in list(user_event_counts.items())[:5]:\n",
        "        print(f\"  - User {user_id}: {counts}\")\n",
        "    print(f\"(Total unique users: {len(user_event_counts)})\")\n",
        "\n",
        "    # Run Task 2 and print the result.\n",
        "    multi_tagged_docs = find_multi_tagged_documents(semi_structured_data)\n",
        "    print(\"\\nTask 2: Valid documents with multiple tags (first 5 found):\")\n",
        "    for doc in multi_tagged_docs[:5]:\n",
        "        print(f\"  - Document ID: {doc['id']}, Tags: {doc['tags']}\")\n",
        "    print(f\"(Total valid documents found: {len(multi_tagged_docs)})\")\n",
        "\n",
        "    # Run Task 3 and print the result.\n",
        "    top_keyword, keyword_count = find_most_frequent_keyword(unstructured_data)\n",
        "    print(f\"\\nTask 3: The most frequent keyword is '{top_keyword}' with a count of {keyword_count}.\")\n",
        "\n",
        "    print(\"\\n--- End of Assignment ---\")\n",
        "    print(\"Feel free to add more test cases or explore the data further!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Big Data Programming Assignment ---\n",
            "The datasets are now being generated synthetically.\n",
            "This simulates the 'Volume' characteristic of big data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-702990684.py:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  timestamp = (datetime.utcnow() - timedelta(seconds=random.randint(0, 3600))).isoformat() + \"Z\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully generated 10000 structured records.\n",
            "Successfully generated 10000 semi-structured records.\n",
            "Successfully generated 10000 unstructured records.\n",
            "\n",
            "--- Your Results ---\n",
            "Task 1: User event counts for the first 5 users:\n",
            "  - User 1000: {'click': 1, 'page_view': 0, 'purchase': 2}\n",
            "  - User 1001: {'click': 3, 'page_view': 3, 'purchase': 5}\n",
            "  - User 1002: {'click': 1, 'page_view': 0, 'purchase': 4}\n",
            "  - User 1003: {'click': 5, 'page_view': 5, 'purchase': 3}\n",
            "  - User 1004: {'click': 1, 'page_view': 2, 'purchase': 2}\n",
            "(Total unique users: 1001)\n",
            "\n",
            "Task 2: Valid documents with multiple tags (first 5 found):\n",
            "  - Document ID: 63aeea5c-4372-4f7b-9758-60264975dd59, Tags: ['data', 'governance', 'trends']\n",
            "  - Document ID: cab43684-5f79-43f8-9f32-17452809d067, Tags: ['big data', 'lecture', '5vs']\n",
            "  - Document ID: 9a2f6beb-3c5a-4704-b1eb-f74b1c900647, Tags: ['programming', 'python']\n",
            "  - Document ID: 20d29388-888f-456f-bc21-8fc9cc708e6a, Tags: ['animal', 'story']\n",
            "  - Document ID: 1abfa8f7-fa56-48ba-b76b-7bbcee8fc9dd, Tags: ['AI', 'analytics', 'learning']\n",
            "(Total valid documents found: 9000)\n",
            "\n",
            "Task 3: The most frequent keyword is 'structured' with a count of 3139.\n",
            "\n",
            "--- End of Assignment ---\n",
            "Feel free to add more test cases or explore the data further!\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "ZydApuP4Bla-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cb1d8b-7b44-41e9-d7dc-38778a1ac464"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}